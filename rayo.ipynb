{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rayo: Your daily news help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A better architecture will be created to select news category later. For now we stick to tech news\n",
    "default_url = \"https://www.bbc.com\"\n",
    "url = \"https://www.bbc.com/news/technology\"\n",
    "'''\n",
    "category = input(\"Enter a category:\")\n",
    "if category == \"technology\":\n",
    "    url = \"https://www.bbc.com/news/technology\"\n",
    "elif category == \"business\":\n",
    "    url = \"https://www.bbc.com/news/business\"\n",
    "elif category == \"entertainment\":\n",
    "    url = \"https://www.bbc.com/news/entertainment_and_arts\"\n",
    "elif category == \"health\":\n",
    "    url = \"https://www.bbc.com/news/health\"\n",
    "elif category == 'science':\n",
    "    url = \"https://www.bbc.com/news/science_and_environment\"\n",
    "elif category == 'sports':\n",
    "    url = \"https://www.bbc.com/sport\"\n",
    "elif category == 'world':\n",
    "    url = \"https://www.bbc.com/news/world\"\n",
    "\n",
    "    '''\n",
    "# Set a time threshold for \"recent\" news (e.g., up to a day)\n",
    "one_day_ago = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the news articles on the page\n",
    "    articles = soup.find_all('a',class_='qa-heading-link lx-stream-post__header-link')\n",
    "\n",
    "    # Initialize lists to store article headlines and links\n",
    "    news_headlines = []\n",
    "    news_links = []\n",
    "\n",
    "    # Loop through the articles and extract relevant information\n",
    "    for article in articles:\n",
    "        link = article.find('href')\n",
    "        \" \".join(article.text.split())\n",
    "        news_headlines.append(article.text)\n",
    "\n",
    "        #Extract link\n",
    "        string_soup = str(article).replace('<html><body>', '').replace('</body></html>', '').replace('<p>', '').replace('</p>', '')\n",
    "        pattern = r'href=\"([^\"]*)\"'\n",
    "        match = re.search(pattern, string_soup)\n",
    "        if match:\n",
    "            extracted_url = match.group(1)\n",
    "            news_links.append(default_url + extracted_url)\n",
    "        else:\n",
    "            print(\"No match found.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "\n",
    "for url in news_links:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = \"\"\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            text += paragraph.get_text() + \" \"\n",
    "        content.append(text)\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_news_data = pd.DataFrame({'Headline': news_headlines, 'Link': news_links, 'Content': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Create a summarization pipeline using the \"facebook/bart-large-cnn\" model\n",
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(content):\n",
    "    summary = pipe(content, max_length=150, min_length=50, do_sample=False)\n",
    "    summarized_text = summary[0]['summary_text']\n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tinder Matchmaker feature allows friends and family to recommend matches. Up to 15 people can view a profile without needing to log in to Tinder. Dating expert Sarah Louise Ryan says there could be privacy concerns. The feature will first be rolled out in 15 countries, including the UK, the US and Australia.\n"
     ]
    }
   ],
   "source": [
    "summarized_text = generate_summary(daily_news_data['Content'][0])\n",
    "print (summarized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
