{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rayo: Your daily news help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  12%|█▏        | 189M/1.63G [16:18<2:04:06, 193kB/s]\n",
      "Downloading pytorch_model.bin:   9%|▉         | 147M/1.63G [07:22<1:14:12, 332kB/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BartConfig, #TFBartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A better architecture will be created to select news category later. For now we stick to tech news\n",
    "default_url = \"https://www.bbc.com\"\n",
    "url = \"https://www.bbc.com/news/technology\"\n",
    "'''\n",
    "category = input(\"Enter a category:\")\n",
    "if category == \"technology\":\n",
    "    url = \"https://www.bbc.com/news/technology\"\n",
    "elif category == \"business\":\n",
    "    url = \"https://www.bbc.com/news/business\"\n",
    "elif category == \"entertainment\":\n",
    "    url = \"https://www.bbc.com/news/entertainment_and_arts\"\n",
    "elif category == \"health\":\n",
    "    url = \"https://www.bbc.com/news/health\"\n",
    "elif category == 'science':\n",
    "    url = \"https://www.bbc.com/news/science_and_environment\"\n",
    "elif category == 'sports':\n",
    "    url = \"https://www.bbc.com/sport\"\n",
    "elif category == 'world':\n",
    "    url = \"https://www.bbc.com/news/world\"\n",
    "\n",
    "    '''\n",
    "# Set a time threshold for \"recent\" news (e.g., up to a day)\n",
    "one_day_ago = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the news articles on the page\n",
    "    articles = soup.find_all('a',class_='qa-heading-link lx-stream-post__header-link')\n",
    "\n",
    "    # Initialize lists to store article headlines and links\n",
    "    news_headlines = []\n",
    "    news_links = []\n",
    "\n",
    "    # Loop through the articles and extract relevant information\n",
    "    for article in articles:\n",
    "        link = article.find('href')\n",
    "        \" \".join(article.text.split())\n",
    "        news_headlines.append(article.text)\n",
    "\n",
    "        #Extract link\n",
    "        string_soup = str(article).replace('<html><body>', '').replace('</body></html>', '').replace('<p>', '').replace('</p>', '')\n",
    "        pattern = r'href=\"([^\"]*)\"'\n",
    "        match = re.search(pattern, string_soup)\n",
    "        if match:\n",
    "            extracted_url = match.group(1)\n",
    "            news_links.append(default_url + extracted_url)\n",
    "        else:\n",
    "            print(\"No match found.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "\n",
    "for url in news_links:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = \"\"\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            text += paragraph.get_text() + \" \"\n",
    "        content.append(text)\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_news_data = pd.DataFrame({'Headline': news_headlines, 'Link': news_links, 'Content': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Polish tech entrepreneur\\'s global project, aimed at getting more children into computer programming, has been endorsed by Pope Francis.  Miron Mironiuk, founder of artificial intelligence company Cosmose AI, is drawing on his own experience of coding transforming his life. He said the \"Code with Pope\" initiative would bridge \"the glaring disparities in education\" across the globe.  It is hoped the Pope\\'s involvement will attract Catholic countries. \"We believe that the involvement of the Pope will help to convince them to spend some time and use this opportunity to learn programming for free,\" Mr Mironiuk told the BBC.  The initiative will champion access to coding education through a free online learning platform for students aged 11-15 across Europe, Africa and Latin America.  After 60 hours of dedicated learning, children will be equipped with the basics of Python, one of the world\\'s most popular coding languages.  In the digital age, programming skills have become as fundamental as reading and writing.  World Economic Forum data released in 2023 revealed that \"the majority of the fastest growing roles are technology-related roles\". However, a severe global shortage of tech skills threatens to leave 85 million job positions unfilled by 2030.  As a result, increasing access to high-quality programming education has become a necessity, particularly in low and middle-income countries - many of which are Catholic.  A large percentage of the Polish population identifies as Catholic. The 33-year-old millionaire Mr Mironiuk told the BBC that he was proud of his Polish heritage and to be part of a crop of successful Polish people working in technology. The country is making significant strides in the tech scene, particularly in AI, with companies like Google Brain, Cosmose AI and Open AI having significant numbers of Polish employees. But Mr Mironiuk is also aware that many countries are not as fortunate, and hopes this educational programme could help change that. The programme will be available in Spanish, English, Italian and Polish. It is expected to reach children all over South America except Brazil, and in English speaking nations in Africa and South East Asia. This is not the first time the Pope has encouraged young people to get into coding, having helped write a line of code for a UN initiative in 2019. Mr Mironiuk will meet the Pope at the Vatican. But he admits he\\'s not anticipating the pontiff to emulate his students in acquiring new skills. \"I don\\'t expect him to know Python very well, at least,\" he said. \"But he will get a certificate for his efforts in helping start the programme.\" You can hear more of Mr Mironiuk on Tech Life on the BBC World Service and BBC Sounds.  Pope suggests Church could bless same-sex couples Pope warns world \\'collapsing\\' due to climate change Gaza hospitals stopping some services as UN warns fuel runs out tonight \\'Heck of a reunion\\': Trump silent as Cohen dishes dirt Republicans pick Mike Johnson as new Speaker nominee The brutal impact of war on Gaza\\'s children. VideoThe brutal impact of war on Gaza\\'s children Tanzanian father\\'s anguish over son missing in Israel Teenage cancer patient: How a bed saved my life Elton John photography collection to go on display \\'Heck of a reunion\\': Trump silent as Cohen dishes dirt Republicans self-destruct again in US Speaker saga \\'Let the games begin\\' - NBA 2023-24 season preview At the scene of deadly US 158-car pile-up. VideoAt the scene of deadly US 158-car pile-up The last residents of a bitterly contested Ukrainian town The town telling influencers to stay away What if a rogue nation dimmed the Sun? Why so many veterinarians suffer © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_news_data['Content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFBartForConditionalGeneration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\Desktop\\Rayo\\rayo.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the pretrained model and tokenizer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m config \u001b[39m=\u001b[39m BartConfig\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mfacebook/bart-large-cnn\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m TFBartForConditionalGeneration(config\u001b[39m=\u001b[39mconfig)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/bart-large-cnn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#model_name = \"facebook/bart-large-cnn\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#tokenizer = BartTokenizer.from_pretrained(model_name)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#model = BartForConditionalGeneration.from_pretrained(model_name)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Input text to summarize\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TFBartForConditionalGeneration' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model and tokenizer\n",
    "config = BartConfig.from_pretrained('facebook/bart-large-cnn')\n",
    "model = TFBartForConditionalGeneration(config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "\n",
    "#model_name = \"facebook/bart-large-cnn\"\n",
    "#tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "#model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Input text to summarize\n",
    "input_text = daily_news_data['Content'][0]\n",
    "\n",
    "# Tokenize and summarize\n",
    "#input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
    "#summary_ids = model.generate(input_ids, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "#summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import TFBartForConditionalGeneration, BartConfig,AutoTokenizer\n",
    "\n",
    "#config = BartConfig.from_pretrained('facebook/bart-large-cnn')\n",
    "#model = TFBartForConditionalGeneration(config=config)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "#summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "#print (summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input text to summarize\n",
    "input_text = daily_news_data['Content'][0]\n",
    "\n",
    "# Tokenize and summarize\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:   1%|▏         | 21.0M/1.63G [00:46<58:47, 455kB/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\Desktop\\Rayo\\rayo.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfacebook/bart-large-cnn\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Tokenize and summarize\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/Rayo/rayo.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(daily_news_data[\u001b[39m'\u001b[39m\u001b[39mContent\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    567\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2929\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2926\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2927\u001b[0m         \u001b[39m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m         filename \u001b[39m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 2929\u001b[0m         resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   2930\u001b[0m             pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs\n\u001b[0;32m   2931\u001b[0m         )\n\u001b[0;32m   2932\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   2933\u001b[0m     \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   2934\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   2935\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2936\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   2937\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   2938\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    430\u001b[0m         path_or_repo_id,\n\u001b[0;32m    431\u001b[0m         filename,\n\u001b[0;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1431\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1431\u001b[0m     http_get(\n\u001b[0;32m   1432\u001b[0m         url_to_download,\n\u001b[0;32m   1433\u001b[0m         temp_file,\n\u001b[0;32m   1434\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1435\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1436\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1437\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1438\u001b[0m     )\n\u001b[0;32m   1440\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1441\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:551\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    541\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    544\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    545\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    550\u001b[0m )\n\u001b[1;32m--> 551\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    553\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 936\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    938\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    939\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[0;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[1;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[0;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[0;32m    883\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    462\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 464\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:   1%|▏         | 21.0M/1.63G [00:59<58:47, 455kB/s]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text2text_generation.SummarizationPipeline object at 0x00000227E5E3BB20>\n"
     ]
    }
   ],
   "source": [
    "# Create a summarization pipeline using the \"facebook/bart-large-cnn\" model\n",
    "#pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "#print (pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text2text_generation.SummarizationPipeline object at 0x00000227B2A4E5F0>\n"
     ]
    }
   ],
   "source": [
    "#from transformers import TFBartForConditionalGeneration, BartConfig,AutoTokenizer\n",
    "\n",
    "#config = BartConfig.from_pretrained('facebook/bart-large-cnn')\n",
    "#model = TFBartForConditionalGeneration(config=config)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "#summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "#print (summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary = pipe(daily_news_data['Content'][0], max_length=150, min_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A new £6.1m residential building equipped with the latest technology is due to open at a college for students with disabilities. The accommodation at National Star College will act as a \"smart house\" and includes voice-activated technology. It will allow students to adapt artificial intelligence (AI) to suit their personal needs.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def generate_summary(content):\n",
    " #   summary = pipe(content, max_length=150, min_length=50, do_sample=False)\n",
    "  #  summarized_text = summary[0]['summary_text']\n",
    "   # return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarized_text = generate_summary(daily_news_data['Content'][0])\n",
    "#print (summarized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
